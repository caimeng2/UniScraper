<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>Uniscraper.Uniscraper API documentation</title>
<meta name="description" content="A scraper that extracts text from multiple types of webpages, including html, pdf, word documents, presentation slides, and spreadsheets" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>Uniscraper.Uniscraper</code></h1>
</header>
<section id="section-intro">
<p>A scraper that extracts text from multiple types of webpages, including html, pdf, word documents, presentation slides, and spreadsheets</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;A scraper that extracts text from multiple types of webpages, including html, pdf, word documents, presentation slides, and spreadsheets&#34;&#34;&#34;
import io
import csv
import time
import pandas as pd
import requests
from bs4 import BeautifulSoup
from bs4.element import Comment
import nltk
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter
from pdfminer.layout import LAParams
from pdfminer.converter import TextConverter
from pdfminer.pdfpage import PDFPage
import docx
from pptx import Presentation

options = Options()
options.add_argument(&#39;--headless&#39;)
driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)

def tag_visible(element):
    &#34;&#34;&#34;A helper function to filter visible tags&#34;&#34;&#34;
    if element.parent.name in [&#39;style&#39;, &#39;script&#39;, &#39;head&#39;, &#39;title&#39;, &#39;meta&#39;, &#39;[document]&#39;]:
        return False
    if isinstance(element, Comment):
        return False
    return True

def pdf_to_text(pdf_file):
    &#34;&#34;&#34;Extract text from pdf file&#34;&#34;&#34;
    text_memory_file = io.StringIO()
    rsrcmgr = PDFResourceManager()
    device = TextConverter(rsrcmgr, text_memory_file, laparams=LAParams())
    interpreter = PDFPageInterpreter(rsrcmgr, device)
    for page in PDFPage.get_pages(pdf_file):
        interpreter.process_page(page)
    text = text_memory_file.getvalue()
    text_memory_file.close()
    return text

def doc_to_text(doc_file):
    &#34;&#34;&#34;Extract text from word document&#34;&#34;&#34;
    doc = docx.Document(doc_file)
    text = []
    for i in range(len(doc.paragraphs)):
        para = doc.paragraphs[i]
        text.append(para.text)
    return &#34;\n\n&#34;.join(text)

def ppt_to_text(ppt_file):
    &#34;&#34;&#34;Extract text from presentation slides&#34;&#34;&#34;
    ppt = Presentation(ppt_file)
    text = []
    for slide in ppt.slides:
        for shape in slide.shapes:
            if hasattr(shape, &#34;text&#34;):
                text.append(shape.text)
    return &#34; &#34;.join(text)

def excel_to_text(excel_file):
    &#34;&#34;&#34;Extract text from spreadsheets&#34;&#34;&#34;
    data = pd.ExcelFile(excel_file)
    text = []
    for i in range(len(data.sheet_names)):
        sheet = data.parse(data.sheet_names[i])
        for content in sheet.values.tolist():
            text.append(content[0])
    return &#34;\n\n&#34;.join(text)

def text_from_url(url):
    &#34;&#34;&#34;Extract text from web pages&#34;&#34;&#34;
    try:
        req = requests.get(url)
        if &#39;text/html&#39; in req.headers[&#34;Content-Type&#34;]:
            driver.get(url)
            time.sleep(15)
            body = driver.page_source
            soup = BeautifulSoup(body, &#39;html.parser&#39;)
            texts = soup.findAll(text=True)
            visible_texts = filter(tag_visible, texts)
            text = u&#34; &#34;.join(t.strip() for t in visible_texts)
        elif &#39;application/pdf&#39; in req.headers[&#34;Content-Type&#34;]:
            pdf_memory_file = io.BytesIO()
            pdf_memory_file.write(req.content)
            text = pdf_to_text(pdf_memory_file)
        elif &#39;application/vnd.openxmlformats-officedocument.wordprocessingml.document&#39; in req.headers[&#34;Content-Type&#34;]:
            word_memory_file = io.BytesIO()
            word_memory_file.write(req.content)
            text = doc_to_text(word_memory_file)
        elif &#39;application/vnd.openxmlformats-officedocument.presentationml.presentation&#39; in req.headers[&#34;Content-Type&#34;]:
            ppt_memory_file = io.BytesIO()
            ppt_memory_file.write(req.content)
            text = ppt_to_text(ppt_memory_file)
        elif &#39;application/vnd.openxmlformats-officedocument.spreadsheetml.sheet&#39; in req.headers[&#34;Content-Type&#34;]:
            excel_memory_file = io.BytesIO()
            excel_memory_file.write(req.content)
            text = excel_to_text(excel_memory_file)
        else:
            csv.writer(open(&#34;error.csv&#34;, &#34;a&#34;)).writerow([&#34;Unsupported content type: &#34;, req.headers[&#34;Content-Type&#34;], url])
            text = &#34;&#34;
    except Exception as err:
        csv.writer(open(&#34;error.csv&#34;, &#34;a&#34;)).writerow([err, url])
        text = &#34;&#34;
    return text

def remove_non_eng(text):
    &#34;&#34;&#34;A helper funtion to remove non-english text&#34;&#34;&#34;
    words = set(nltk.corpus.words.words())
    english = &#34; &#34;.join(w for w in nltk.wordpunct_tokenize(text) if w.lower() in words or not w.isalpha())
    return english

def paragraph_from_text(text, search_string):
    &#34;&#34;&#34;Get paragraphs containing a keyword with its context&#34;&#34;&#34;
    split_text = text.lower().split(&#34;\n\n&#34;)
    found = False
    previous = &#34;&#34;
    para = []
    # to extract paragraphs containing a keyword and its immediate previous and after paragraphs
    for p in split_text:
        if found:
            para.append(p)
            found = False
        if p.find(search_string.lower()) != -1:
            para.append(previous)
            para.append(p)
            found = True
        else:
            found = False
        previous = p
    para = list(set(para)) # remove duplicates
    para_str = &#34; &#34;.join(para)
    return para_str

class uniscraper():
    &#34;&#34;&#34;A scraper that works with multiple types of webpages&#34;&#34;&#34;
    def __init__(self, url):
        self.url = url
        self.text = text_from_url(url)
        self.english = remove_non_eng(self.text)
        self.para = None
    def search(self, string):
        &#34;&#34;&#34;Search text for a keyword&#34;&#34;&#34;
        self.para = paragraph_from_text(self.text, string)
        return self.para</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="Uniscraper.Uniscraper.doc_to_text"><code class="name flex">
<span>def <span class="ident">doc_to_text</span></span>(<span>doc_file)</span>
</code></dt>
<dd>
<div class="desc"><p>Extract text from word document</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def doc_to_text(doc_file):
    &#34;&#34;&#34;Extract text from word document&#34;&#34;&#34;
    doc = docx.Document(doc_file)
    text = []
    for i in range(len(doc.paragraphs)):
        para = doc.paragraphs[i]
        text.append(para.text)
    return &#34;\n\n&#34;.join(text)</code></pre>
</details>
</dd>
<dt id="Uniscraper.Uniscraper.excel_to_text"><code class="name flex">
<span>def <span class="ident">excel_to_text</span></span>(<span>excel_file)</span>
</code></dt>
<dd>
<div class="desc"><p>Extract text from spreadsheets</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def excel_to_text(excel_file):
    &#34;&#34;&#34;Extract text from spreadsheets&#34;&#34;&#34;
    data = pd.ExcelFile(excel_file)
    text = []
    for i in range(len(data.sheet_names)):
        sheet = data.parse(data.sheet_names[i])
        for content in sheet.values.tolist():
            text.append(content[0])
    return &#34;\n\n&#34;.join(text)</code></pre>
</details>
</dd>
<dt id="Uniscraper.Uniscraper.paragraph_from_text"><code class="name flex">
<span>def <span class="ident">paragraph_from_text</span></span>(<span>text, search_string)</span>
</code></dt>
<dd>
<div class="desc"><p>Get paragraphs containing a keyword with its context</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def paragraph_from_text(text, search_string):
    &#34;&#34;&#34;Get paragraphs containing a keyword with its context&#34;&#34;&#34;
    split_text = text.lower().split(&#34;\n\n&#34;)
    found = False
    previous = &#34;&#34;
    para = []
    # to extract paragraphs containing a keyword and its immediate previous and after paragraphs
    for p in split_text:
        if found:
            para.append(p)
            found = False
        if p.find(search_string.lower()) != -1:
            para.append(previous)
            para.append(p)
            found = True
        else:
            found = False
        previous = p
    para = list(set(para)) # remove duplicates
    para_str = &#34; &#34;.join(para)
    return para_str</code></pre>
</details>
</dd>
<dt id="Uniscraper.Uniscraper.pdf_to_text"><code class="name flex">
<span>def <span class="ident">pdf_to_text</span></span>(<span>pdf_file)</span>
</code></dt>
<dd>
<div class="desc"><p>Extract text from pdf file</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pdf_to_text(pdf_file):
    &#34;&#34;&#34;Extract text from pdf file&#34;&#34;&#34;
    text_memory_file = io.StringIO()
    rsrcmgr = PDFResourceManager()
    device = TextConverter(rsrcmgr, text_memory_file, laparams=LAParams())
    interpreter = PDFPageInterpreter(rsrcmgr, device)
    for page in PDFPage.get_pages(pdf_file):
        interpreter.process_page(page)
    text = text_memory_file.getvalue()
    text_memory_file.close()
    return text</code></pre>
</details>
</dd>
<dt id="Uniscraper.Uniscraper.ppt_to_text"><code class="name flex">
<span>def <span class="ident">ppt_to_text</span></span>(<span>ppt_file)</span>
</code></dt>
<dd>
<div class="desc"><p>Extract text from presentation slides</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ppt_to_text(ppt_file):
    &#34;&#34;&#34;Extract text from presentation slides&#34;&#34;&#34;
    ppt = Presentation(ppt_file)
    text = []
    for slide in ppt.slides:
        for shape in slide.shapes:
            if hasattr(shape, &#34;text&#34;):
                text.append(shape.text)
    return &#34; &#34;.join(text)</code></pre>
</details>
</dd>
<dt id="Uniscraper.Uniscraper.remove_non_eng"><code class="name flex">
<span>def <span class="ident">remove_non_eng</span></span>(<span>text)</span>
</code></dt>
<dd>
<div class="desc"><p>A helper funtion to remove non-english text</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_non_eng(text):
    &#34;&#34;&#34;A helper funtion to remove non-english text&#34;&#34;&#34;
    words = set(nltk.corpus.words.words())
    english = &#34; &#34;.join(w for w in nltk.wordpunct_tokenize(text) if w.lower() in words or not w.isalpha())
    return english</code></pre>
</details>
</dd>
<dt id="Uniscraper.Uniscraper.tag_visible"><code class="name flex">
<span>def <span class="ident">tag_visible</span></span>(<span>element)</span>
</code></dt>
<dd>
<div class="desc"><p>A helper function to filter visible tags</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tag_visible(element):
    &#34;&#34;&#34;A helper function to filter visible tags&#34;&#34;&#34;
    if element.parent.name in [&#39;style&#39;, &#39;script&#39;, &#39;head&#39;, &#39;title&#39;, &#39;meta&#39;, &#39;[document]&#39;]:
        return False
    if isinstance(element, Comment):
        return False
    return True</code></pre>
</details>
</dd>
<dt id="Uniscraper.Uniscraper.text_from_url"><code class="name flex">
<span>def <span class="ident">text_from_url</span></span>(<span>url)</span>
</code></dt>
<dd>
<div class="desc"><p>Extract text from web pages</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def text_from_url(url):
    &#34;&#34;&#34;Extract text from web pages&#34;&#34;&#34;
    try:
        req = requests.get(url)
        if &#39;text/html&#39; in req.headers[&#34;Content-Type&#34;]:
            driver.get(url)
            time.sleep(15)
            body = driver.page_source
            soup = BeautifulSoup(body, &#39;html.parser&#39;)
            texts = soup.findAll(text=True)
            visible_texts = filter(tag_visible, texts)
            text = u&#34; &#34;.join(t.strip() for t in visible_texts)
        elif &#39;application/pdf&#39; in req.headers[&#34;Content-Type&#34;]:
            pdf_memory_file = io.BytesIO()
            pdf_memory_file.write(req.content)
            text = pdf_to_text(pdf_memory_file)
        elif &#39;application/vnd.openxmlformats-officedocument.wordprocessingml.document&#39; in req.headers[&#34;Content-Type&#34;]:
            word_memory_file = io.BytesIO()
            word_memory_file.write(req.content)
            text = doc_to_text(word_memory_file)
        elif &#39;application/vnd.openxmlformats-officedocument.presentationml.presentation&#39; in req.headers[&#34;Content-Type&#34;]:
            ppt_memory_file = io.BytesIO()
            ppt_memory_file.write(req.content)
            text = ppt_to_text(ppt_memory_file)
        elif &#39;application/vnd.openxmlformats-officedocument.spreadsheetml.sheet&#39; in req.headers[&#34;Content-Type&#34;]:
            excel_memory_file = io.BytesIO()
            excel_memory_file.write(req.content)
            text = excel_to_text(excel_memory_file)
        else:
            csv.writer(open(&#34;error.csv&#34;, &#34;a&#34;)).writerow([&#34;Unsupported content type: &#34;, req.headers[&#34;Content-Type&#34;], url])
            text = &#34;&#34;
    except Exception as err:
        csv.writer(open(&#34;error.csv&#34;, &#34;a&#34;)).writerow([err, url])
        text = &#34;&#34;
    return text</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="Uniscraper.Uniscraper.uniscraper"><code class="flex name class">
<span>class <span class="ident">uniscraper</span></span>
<span>(</span><span>url)</span>
</code></dt>
<dd>
<div class="desc"><p>A scraper that works with multiple types of webpages</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class uniscraper():
    &#34;&#34;&#34;A scraper that works with multiple types of webpages&#34;&#34;&#34;
    def __init__(self, url):
        self.url = url
        self.text = text_from_url(url)
        self.english = remove_non_eng(self.text)
        self.para = None
    def search(self, string):
        &#34;&#34;&#34;Search text for a keyword&#34;&#34;&#34;
        self.para = paragraph_from_text(self.text, string)
        return self.para</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="Uniscraper.Uniscraper.uniscraper.search"><code class="name flex">
<span>def <span class="ident">search</span></span>(<span>self, string)</span>
</code></dt>
<dd>
<div class="desc"><p>Search text for a keyword</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def search(self, string):
    &#34;&#34;&#34;Search text for a keyword&#34;&#34;&#34;
    self.para = paragraph_from_text(self.text, string)
    return self.para</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="Uniscraper" href="index.html">Uniscraper</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="Uniscraper.Uniscraper.doc_to_text" href="#Uniscraper.Uniscraper.doc_to_text">doc_to_text</a></code></li>
<li><code><a title="Uniscraper.Uniscraper.excel_to_text" href="#Uniscraper.Uniscraper.excel_to_text">excel_to_text</a></code></li>
<li><code><a title="Uniscraper.Uniscraper.paragraph_from_text" href="#Uniscraper.Uniscraper.paragraph_from_text">paragraph_from_text</a></code></li>
<li><code><a title="Uniscraper.Uniscraper.pdf_to_text" href="#Uniscraper.Uniscraper.pdf_to_text">pdf_to_text</a></code></li>
<li><code><a title="Uniscraper.Uniscraper.ppt_to_text" href="#Uniscraper.Uniscraper.ppt_to_text">ppt_to_text</a></code></li>
<li><code><a title="Uniscraper.Uniscraper.remove_non_eng" href="#Uniscraper.Uniscraper.remove_non_eng">remove_non_eng</a></code></li>
<li><code><a title="Uniscraper.Uniscraper.tag_visible" href="#Uniscraper.Uniscraper.tag_visible">tag_visible</a></code></li>
<li><code><a title="Uniscraper.Uniscraper.text_from_url" href="#Uniscraper.Uniscraper.text_from_url">text_from_url</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="Uniscraper.Uniscraper.uniscraper" href="#Uniscraper.Uniscraper.uniscraper">uniscraper</a></code></h4>
<ul class="">
<li><code><a title="Uniscraper.Uniscraper.uniscraper.search" href="#Uniscraper.Uniscraper.uniscraper.search">search</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>